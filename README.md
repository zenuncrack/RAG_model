
<body>

<h1>ğŸ“˜ RAG Assistant (Retrieval Augmented Generation)</h1>
<p>
A powerful interactive RAG application built with 
<strong>Streamlit</strong>, <strong>LangChain</strong>, <strong>LangGraph</strong>,
<strong>Groq LLaMA 3.3</strong>, and <strong>ChromaDB</strong>.
Users can upload <strong>PDF</strong> or <strong>TXT</strong> files and ask questions,
and the agent responds using both <strong>document retrieval</strong> and optional 
<strong>web search</strong> tools.
</p>

<hr>

<h2>ğŸš€ Features</h2>
<ul>
    <li>ğŸ“„ Upload <strong>PDF</strong> or <strong>TXT</strong> files</li>
    <li>ğŸ” Content is split into chunks and indexed using <strong>Chroma VectorStore</strong></li>
    <li>ğŸ§  <strong>Custom retriever tool</strong> for answering from uploaded documents</li>
    <li>ğŸŒ Optional <strong>web search tool</strong> for external information</li>
    <li>ğŸ¤– <strong>Groq LLaMA 3.3</strong> for ultra-fast inference</li>
    <li>ğŸ” <strong>LangGraph</strong> tool-calling workflow (LLM â†’ Tool â†’ LLM)</li>
    <li>ğŸ’¬ Clean chat UI using Streamlit</li>
    <li>ğŸ—‘ï¸ One-click <strong>Clear Chat History</strong></li>
</ul>

<hr>

<h2>ğŸ“ Project Structure</h2>

<pre>
â”œâ”€â”€ app.py                   # Streamlit UI
â”œâ”€â”€ rag_model.py             # RAG model with LangGraph + tools
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.html              # This file
</pre>

<hr>

<h2>âš™ï¸ How It Works</h2>

<h3>1ï¸âƒ£ File Upload</h3>
<p>User uploads a PDF or TXT file. The file is temporarily saved and passed into:</p>

<pre><code>build_rag_agent_from_file(file_path)</code></pre>

<h3>2ï¸âƒ£ Text Processing</h3>
<ul>
    <li>Loads the document</li>
    <li>Splits into chunks (RecursiveCharacterTextSplitter)</li>
    <li>Creates embeddings (MiniLM-L6-v2)</li>
    <li>Stores chunks in <strong>Chroma</strong> (in-memory)</li>
</ul>

<h3>3ï¸âƒ£ RAG Tools</h3>
<ul>
    <li><strong>retriever_tool</strong> â†’ Retrieves PDF/TXT chunks</li>
    <li><strong>web_search_tool</strong> â†’ (Optional) Tavily Search API</li>
</ul>

<h3>4ï¸âƒ£ LangGraph Execution Flow</h3>

<pre>
User Message
    â†“
LLM Node (call_llm)
    â†“
Should Continue?
    â†“ YES (LLM requested a tool)
Tool Node (take_action)
    â†“
LLM
    â†“
Final Answer
</pre>

<hr>

<h2>ğŸ’» Running the App</h2>

<h3>1ï¸âƒ£ Install dependencies</h3>
<pre><code>pip install -r requirements.txt</code></pre>

<h3>2ï¸âƒ£ Run Streamlit</h3>
<pre><code>streamlit run app.py</code></pre>

<h3>3ï¸âƒ£ Upload file and chat</h3>
- Upload a PDF or TXT document  
- Ask questions  
- RAG Agent retrieves knowledge  
- Groq LLM generates answers  

<hr>

<h2>ğŸ“¦ Example Code Snippet (Streamlit)</h2>

<pre><code>
if uploaded_file:
    temp_path = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf").name
    with open(temp_path, "wb") as f:
        f.write(uploaded_file.read())
    rag_agent = build_rag_agent_from_file(temp_path)

result = rag_agent.invoke({"messages": [HumanMessage(content=user_input)]})
</code></pre>

<hr>

<h2>ğŸ“Š RAG Graph Diagram</h2>

<pre>
                [User Question]
                        |
                        v
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  LLM Node       â”‚
               â”‚  (call_llm)     â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      |
            Should Continue?
                YES |  NO
                      v
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  Tool Node      â”‚
               â”‚ (take_action)   â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      |
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         |                           |
 [retriever_tool]           [web_search_tool]
         |                           |
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      |
                      v
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  LLM Node       â”‚
               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      |
                   [Answer]
</pre>

<hr>

<h2>ğŸ”§ Requirements</h2>
<p>Create a <code>requirements.txt</code> file with:</p>

<pre><code>
streamlit
langchain
langchain-community
langchain-core
langgraph
langchain-groq
chromadb
sentence-transformers
pypdf
python-dotenv
tavily-python
</code></pre>

<hr>

<h2>ğŸ“œ License</h2>
<p>This project is open-source and free to modify.</p>

</body>
</html>
